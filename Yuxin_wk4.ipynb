{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Linear Regression"
      ],
      "metadata": {
        "id": "zrOGOCVrrtLj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoIVSMsNqDSx",
        "outputId": "783d32f8-f60f-425f-da73-a31a9cbda197"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "# CORE_PATH = '/content/drive/My Drive/babe/GenHack'\n",
        "CORE_PATH = '/content/drive/MyDrive/GenHack'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#by day regression\n",
        "df_lr = pd.read_csv('/content/drive/MyDrive/GenHack/df_lr1201.csv')"
      ],
      "metadata": {
        "id": "Z0t_uh9vqTJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_J17lo5qX6x",
        "outputId": "b05c9cc3-5da2-4f57-9a2e-7c1115643d4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# --- CLEAN DATA ---\n",
        "df_lr_cleaned = df_lr.dropna(\n",
        "    subset=['diff_in_temp', 'mean_NDVI_transformed', 'HGHT', 'is_summer', 'STAID', 'DATE_x']\n",
        ").copy()\n",
        "\n",
        "X = df_lr_cleaned[['mean_NDVI_transformed', 'HGHT', 'is_summer']]\n",
        "X['NVDI_squared'] = X['mean_NDVI_transformed'] ** 2\n",
        "#x1: mean_NDVI_transformed, x2: HGHT, x3: is_summer, x4: NVDI_squared\n",
        "y = df_lr_cleaned['diff_in_temp']\n",
        "\n",
        "# --- TRAIN–TEST SPLIT ---\n",
        "X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(\n",
        "    X, y, df_lr_cleaned.index, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# --- SCALE FEATURES ---\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# --- SKLEARN REGRESSION ---\n",
        "model = LinearRegression()\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"Coefficients:\", model.coef_)\n",
        "print(\"Intercept:\", model.intercept_)\n",
        "\n",
        "# ============================================================\n",
        "# --- PREP GROUPING VARIABLES FOR CLUSTERED SE ---\n",
        "# Avoid dtype issues by converting to int-coded categories\n",
        "# ============================================================\n",
        "\n",
        "station_ids = df_lr_cleaned.loc[idx_train, 'STAID']\n",
        "date_ids = df_lr_cleaned.loc[idx_train, 'DATE_x']\n",
        "\n",
        "# Convert to safe integer codes\n",
        "station_groups = station_ids.astype(\"category\").cat.codes.astype(\"int64\")\n",
        "date_groups = date_ids.astype(\"category\").cat.codes.astype(\"int64\")\n",
        "\n",
        "# --- DESIGN MATRIX FOR STATSMODELS ---\n",
        "X_train_df = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
        "X_train_sm = sm.add_constant(X_train_df)\n",
        "\n",
        "# ============================================================\n",
        "# --- FIT MODEL WITH CLUSTERED STANDARD ERRORS ---\n",
        "# One-way clustering: by station\n",
        "# ============================================================\n",
        "ols_cluster_station = sm.OLS(y_train, X_train_sm).fit(\n",
        "    cov_type=\"cluster\",\n",
        "    cov_kwds={\"groups\": station_groups}\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# --- TWO-WAY CLUSTERING (station + date) ---\n",
        "# Statsmodels: pass Nx2 matrix\n",
        "# ============================================================\n",
        "groups_two_way = np.column_stack([station_groups, date_groups])\n",
        "\n",
        "ols_cluster_tw = sm.OLS(y_train, X_train_sm).fit(\n",
        "    cov_type=\"cluster\",\n",
        "    cov_kwds={\"groups\": groups_two_way}\n",
        ")\n",
        "\n",
        "# --- Print both for comparison ---\n",
        "print(\"\\n=== OLS with Station Clustering ===\")\n",
        "print(ols_cluster_station.summary())\n",
        "\n",
        "print(\"\\n=== OLS with Two-Way Clustering (Station + Date) ===\")\n",
        "print(ols_cluster_tw.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0k9bFdtVqYWr",
        "outputId": "d8db05da-5d82-4a75-ad17-0bb439cc38a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-166593003.py:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X['NVDI_squared'] = X['mean_NDVI_transformed'] ** 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients: [-1.45692278  0.77772055  0.14007904  1.33675371]\n",
            "Intercept: 1.5366082923614992\n",
            "\n",
            "=== OLS with Station Clustering ===\n",
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:           diff_in_temp   R-squared:                       0.167\n",
            "Model:                            OLS   Adj. R-squared:                  0.167\n",
            "Method:                 Least Squares   F-statistic:                     12.48\n",
            "Date:                Wed, 03 Dec 2025   Prob (F-statistic):           1.82e-06\n",
            "Time:                        05:25:22   Log-Likelihood:                -79582.\n",
            "No. Observations:               39458   AIC:                         1.592e+05\n",
            "Df Residuals:                   39453   BIC:                         1.592e+05\n",
            "Df Model:                           4                                         \n",
            "Covariance Type:              cluster                                         \n",
            "=========================================================================================\n",
            "                            coef    std err          z      P>|z|      [0.025      0.975]\n",
            "-----------------------------------------------------------------------------------------\n",
            "const                     1.5366      0.196      7.846      0.000       1.153       1.920\n",
            "mean_NDVI_transformed    -1.4569      0.713     -2.043      0.041      -2.855      -0.059\n",
            "HGHT                      0.7777      0.490      1.587      0.112      -0.183       1.738\n",
            "is_summer                 0.1401      0.039      3.590      0.000       0.064       0.217\n",
            "NVDI_squared              1.3368      0.610      2.191      0.028       0.141       2.533\n",
            "==============================================================================\n",
            "Omnibus:                     4003.915   Durbin-Watson:                   2.008\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            21559.363\n",
            "Skew:                           0.347   Prob(JB):                         0.00\n",
            "Kurtosis:                       6.554   Cond. No.                         10.5\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors are robust to cluster correlation (cluster)\n",
            "\n",
            "=== OLS with Two-Way Clustering (Station + Date) ===\n",
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:           diff_in_temp   R-squared:                       0.167\n",
            "Model:                            OLS   Adj. R-squared:                  0.167\n",
            "Method:                 Least Squares   F-statistic:                     11.70\n",
            "Date:                Wed, 03 Dec 2025   Prob (F-statistic):           3.44e-06\n",
            "Time:                        05:25:22   Log-Likelihood:                -79582.\n",
            "No. Observations:               39458   AIC:                         1.592e+05\n",
            "Df Residuals:                   39453   BIC:                         1.592e+05\n",
            "Df Model:                           4                                         \n",
            "Covariance Type:              cluster                                         \n",
            "=========================================================================================\n",
            "                            coef    std err          z      P>|z|      [0.025      0.975]\n",
            "-----------------------------------------------------------------------------------------\n",
            "const                     1.5366      0.196      7.829      0.000       1.152       1.921\n",
            "mean_NDVI_transformed    -1.4569      0.713     -2.043      0.041      -2.855      -0.059\n",
            "HGHT                      0.7777      0.490      1.588      0.112      -0.182       1.738\n",
            "is_summer                 0.1401      0.040      3.475      0.001       0.061       0.219\n",
            "NVDI_squared              1.3368      0.610      2.190      0.029       0.140       2.533\n",
            "==============================================================================\n",
            "Omnibus:                     4003.915   Durbin-Watson:                   2.008\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            21559.363\n",
            "Skew:                           0.347   Prob(JB):                         0.00\n",
            "Kurtosis:                       6.554   Cond. No.                         10.5\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors are robust to cluster correlation (cluster)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#by quarter regression\n",
        "df_temp = pd.read_csv('/content/drive/MyDrive/GenHack/temp_diff_dpvar.csv')\n",
        "\n",
        "df_temp['DATE'] = pd.to_datetime(df_temp['DATE'])\n",
        "df_temp['quarter_idx'] = df_temp['DATE'].dt.year * 100 + np.where(\n",
        "    df_temp['DATE'].dt.month <= 3,\n",
        "    1,\n",
        "    np.where(\n",
        "        df_temp['DATE'].dt.month <= 6,\n",
        "        2,\n",
        "        np.where(\n",
        "            df_temp['DATE'].dt.month <= 9,\n",
        "            3,\n",
        "            4\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "df_temp_quarter = df_temp.groupby(\n",
        "    by = ['STAID', 'quarter_idx']\n",
        ")[['HGHT', 'diff_in_temp']].mean().reset_index() # Corrected column selection and added .mean() for aggregation\n",
        "df_temp_quarter = df_temp_quarter[df_temp_quarter['diff_in_temp'].notna()]\n",
        "df_temp_quarter['if_summer'] = np.where(\n",
        "    df_temp_quarter['quarter_idx'] % 10 == 3,\n",
        "    1,\n",
        "    0\n",
        ")\n",
        "df_temp_quarter['NVDI_squared'] = df_temp_quarter['diff_in_temp'] ** 2\n",
        "\n",
        "\n",
        "ndvi_df = pd.read_csv('/content/drive/MyDrive/GenHack/vegetation_around_stations.csv')\n",
        "ndvi_df['quarter_idx'] = (ndvi_df['DATE'] // 10000) * 100 + np.where(\n",
        "   (ndvi_df['DATE'] // 100) - (ndvi_df['DATE'] // 10000) * 100 <= 3,\n",
        "   1,\n",
        "   np.where(\n",
        "       (ndvi_df['DATE'] // 100) - (ndvi_df['DATE'] // 10000) * 100 <= 6,\n",
        "       2,\n",
        "       np.where(\n",
        "           (ndvi_df['DATE'] // 100) - (ndvi_df['DATE'] // 10000) * 100 <= 9,\n",
        "           3,\n",
        "           4\n",
        "       )\n",
        "   )\n",
        ")\n",
        "ndvi_df['STAID'] = ndvi_df['STAID'].astype(str)\n",
        "\n",
        "ndvi_df['STAID'] = ndvi_df['STAID'].astype(int)\n",
        "df_lr_qt = pd.merge(\n",
        "    df_temp_quarter,\n",
        "    ndvi_df[['STAID', 'quarter_idx', 'mean_NDVI_transformed']],\n",
        "    on = ['STAID', 'quarter_idx'],\n",
        "    how = 'left'\n",
        ")\n",
        "df_lr_qt = df_lr_qt[df_lr_qt['mean_NDVI_transformed'].notna()]"
      ],
      "metadata": {
        "id": "F0HoIS-TqVHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CLEAN DATA ---\n",
        "df_lr_cleaned_qr = df_lr_qt.dropna(\n",
        "    subset=['diff_in_temp', 'mean_NDVI_transformed', 'HGHT', 'if_summer', 'STAID', 'quarter_idx']\n",
        ").copy()\n",
        "\n",
        "X = df_lr_cleaned_qr[['mean_NDVI_transformed', 'HGHT', 'if_summer']]\n",
        "X['NVDI_squared'] = X['mean_NDVI_transformed'] ** 2\n",
        "#x1: mean_NDVI_transformed, x2: HGHT, x3: is_summer, x4: NVDI_squared\n",
        "y = df_lr_cleaned_qr['diff_in_temp']\n",
        "\n",
        "# --- TRAIN–TEST SPLIT ---\n",
        "X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(\n",
        "    X, y, df_lr_cleaned_qr.index, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# --- SCALE FEATURES ---\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# --- SKLEARN REGRESSION ---\n",
        "model = LinearRegression()\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"Coefficients:\", model.coef_)\n",
        "print(\"Intercept:\", model.intercept_)\n",
        "\n",
        "# ============================================================\n",
        "# --- PREP GROUPING VARIABLES FOR CLUSTERED SE ---\n",
        "# Avoid dtype issues by converting to int-coded categories\n",
        "# ============================================================\n",
        "\n",
        "station_ids = df_lr_cleaned_qr.loc[idx_train, 'STAID']\n",
        "date_ids = df_lr_cleaned_qr.loc[idx_train, 'quarter_idx']\n",
        "\n",
        "\n",
        "# Convert to safe integer codes\n",
        "station_groups = station_ids.astype(\"category\").cat.codes.astype(\"int64\")\n",
        "date_groups = date_ids.astype(\"category\").cat.codes.astype(\"int64\")\n",
        "\n",
        "# --- DESIGN MATRIX FOR STATSMODELS ---\n",
        "X_train_df = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
        "X_train_sm = sm.add_constant(X_train_df)\n",
        "\n",
        "# ============================================================\n",
        "# --- FIT MODEL WITH CLUSTERED STANDARD ERRORS ---\n",
        "# One-way clustering: by station\n",
        "# ============================================================\n",
        "ols_cluster_date = sm.OLS(y_train, X_train_sm).fit(\n",
        "    cov_type=\"cluster\",\n",
        "    cov_kwds={\"groups\": date_groups}\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# --- TWO-WAY CLUSTERING (station + date) ---\n",
        "# Statsmodels: pass Nx2 matrix\n",
        "# ============================================================\n",
        "groups_two_way = np.column_stack([station_groups, date_groups])\n",
        "\n",
        "ols_cluster_tw = sm.OLS(y_train, X_train_sm).fit(\n",
        "    cov_type=\"cluster\",\n",
        "    cov_kwds={\"groups\": groups_two_way}\n",
        ")\n",
        "\n",
        "# --- Print both for comparison ---\n",
        "print(\"\\n=== OLS with Quarter Clustering ===\")\n",
        "print(ols_cluster_date.summary())\n",
        "\n",
        "print(\"\\n=== OLS with Two-Way Clustering (Station + Date) ===\")\n",
        "print(ols_cluster_tw.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8NFfbnAq1ax",
        "outputId": "7c8b5fd2-30fd-444c-d69d-5bf7ffd46e0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients: [-1.77850166  0.90669861  0.12385409  1.57511016]\n",
            "Intercept: 1.6127793112128612\n",
            "\n",
            "=== OLS with Quarter Clustering ===\n",
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:           diff_in_temp   R-squared:                       0.340\n",
            "Model:                            OLS   Adj. R-squared:                  0.334\n",
            "Method:                 Least Squares   F-statistic:                     159.5\n",
            "Date:                Wed, 03 Dec 2025   Prob (F-statistic):           1.65e-11\n",
            "Time:                        05:27:32   Log-Likelihood:                -735.55\n",
            "No. Observations:                 433   AIC:                             1481.\n",
            "Df Residuals:                     428   BIC:                             1501.\n",
            "Df Model:                           4                                         \n",
            "Covariance Type:              cluster                                         \n",
            "=========================================================================================\n",
            "                            coef    std err          z      P>|z|      [0.025      0.975]\n",
            "-----------------------------------------------------------------------------------------\n",
            "const                     1.6128      0.063     25.648      0.000       1.490       1.736\n",
            "mean_NDVI_transformed    -1.7785      0.328     -5.422      0.000      -2.421      -1.136\n",
            "HGHT                      0.9067      0.040     22.650      0.000       0.828       0.985\n",
            "if_summer                 0.1239      0.051      2.409      0.016       0.023       0.225\n",
            "NVDI_squared              1.5751      0.307      5.137      0.000       0.974       2.176\n",
            "==============================================================================\n",
            "Omnibus:                      151.267   Durbin-Watson:                   2.006\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              708.762\n",
            "Skew:                           1.458   Prob(JB):                    1.24e-154\n",
            "Kurtosis:                       8.548   Cond. No.                         10.3\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors are robust to cluster correlation (cluster)\n",
            "\n",
            "=== OLS with Two-Way Clustering (Station + Date) ===\n",
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:           diff_in_temp   R-squared:                       0.340\n",
            "Model:                            OLS   Adj. R-squared:                  0.334\n",
            "Method:                 Least Squares   F-statistic:                    -12.80\n",
            "Date:                Wed, 03 Dec 2025   Prob (F-statistic):               1.00\n",
            "Time:                        05:27:32   Log-Likelihood:                -735.55\n",
            "No. Observations:                 433   AIC:                             1481.\n",
            "Df Residuals:                     428   BIC:                             1501.\n",
            "Df Model:                           4                                         \n",
            "Covariance Type:              cluster                                         \n",
            "=========================================================================================\n",
            "                            coef    std err          z      P>|z|      [0.025      0.975]\n",
            "-----------------------------------------------------------------------------------------\n",
            "const                     1.6128      0.219      7.356      0.000       1.183       2.042\n",
            "mean_NDVI_transformed    -1.7785      0.703     -2.530      0.011      -3.156      -0.401\n",
            "HGHT                      0.9067      0.523      1.734      0.083      -0.118       1.931\n",
            "if_summer                 0.1239      0.019      6.399      0.000       0.086       0.162\n",
            "NVDI_squared              1.5751      0.610      2.584      0.010       0.381       2.770\n",
            "==============================================================================\n",
            "Omnibus:                      151.267   Durbin-Watson:                   2.006\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              708.762\n",
            "Skew:                           1.458   Prob(JB):                    1.24e-154\n",
            "Kurtosis:                       8.548   Cond. No.                         10.3\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors are robust to cluster correlation (cluster)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1798019389.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X['NVDI_squared'] = X['mean_NDVI_transformed'] ** 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GMB for regression"
      ],
      "metadata": {
        "id": "_-RE1HAarxYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- Ensure data is sorted by date ---\n",
        "# Adjust \"DATE_X\" if your date column has a different name\n",
        "df_sorted = df_lr_cleaned_qr.sort_values(\"quarter_idx\").copy()\n",
        "\n",
        "# --- Define Features (X) and Target (y) ---\n",
        "X = df_sorted[['mean_NDVI_transformed', 'HGHT', 'if_summer', 'NVDI_squared']]\n",
        "y = df_sorted['diff_in_temp']\n",
        "\n",
        "# --- TIME-BASED TRAIN–TEST SPLIT (BY DATE) ---\n",
        "# Use unique dates so the split is on whole days, not random rows\n",
        "unique_dates = df_sorted[\"quarter_idx\"].sort_values().unique()\n",
        "cutoff_idx = int(len(unique_dates) * 0.8)\n",
        "cutoff_date = unique_dates[cutoff_idx]\n",
        "\n",
        "train_mask = df_sorted[\"quarter_idx\"] <= cutoff_date\n",
        "test_mask  = df_sorted[\"quarter_idx\"] > cutoff_date\n",
        "\n",
        "X_train_gbm = X[train_mask]\n",
        "X_test_gbm  = X[test_mask]\n",
        "y_train_gbm = y[train_mask]\n",
        "y_test_gbm  = y[test_mask]\n",
        "\n",
        "print(f\"Train period: {df_sorted.loc[train_mask, 'quarter_idx'].min()} → {df_sorted.loc[train_mask, 'quarter_idx'].max()}\")\n",
        "print(f\"Test  period: {df_sorted.loc[test_mask, 'quarter_idx'].min()} → {df_sorted.loc[test_mask, 'quarter_idx'].max()}\")\n",
        "\n",
        "# --- SCALE FEATURES ---\n",
        "scaler_gbm = StandardScaler()\n",
        "X_train_scaled_gbm = scaler_gbm.fit_transform(X_train_gbm)\n",
        "X_test_scaled_gbm = scaler_gbm.transform(X_test_gbm)\n",
        "\n",
        "# --- GRADIENT BOOSTING REGRESSOR MODEL: the \"SAFE\" GBM ---\n",
        "gbm_model = GradientBoostingRegressor(\n",
        "    n_estimators=500,\n",
        "    learning_rate=0.01,\n",
        "    max_depth=2,          # a weak learner would suffice\n",
        "    subsample=0.6,        # stochastic boosting\n",
        "    min_samples_leaf=20,  # each terminal node has at least 20 training points\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "gbm_model.fit(X_train_scaled_gbm, y_train_gbm)\n",
        "\n",
        "# --- Make Predictions ---\n",
        "y_pred_gbm = gbm_model.predict(X_test_scaled_gbm)\n",
        "\n",
        "# --- Evaluate Model ---\n",
        "r2 = r2_score(y_test_gbm, y_pred_gbm)\n",
        "mse = mean_squared_error(y_test_gbm, y_pred_gbm)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "print(f\"\\nGBM Model Evaluation:\")\n",
        "print(f\"R-squared: {r2:.4f}\")\n",
        "print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "print(f\"Root Mean Squared Error: {rmse:.4f}\")\n",
        "\n",
        "# Optional: Feature importances\n",
        "feature_importances = pd.Series(gbm_model.feature_importances_, index=X.columns)\n",
        "print(\"\\nFeature Importances:\")\n",
        "formatted_importances = feature_importances.sort_values(ascending=False).apply(lambda x: '{:.4f}'.format(x))\n",
        "print(formatted_importances)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "out7vKr-r4rQ",
        "outputId": "e0451dd4-c0db-4064-a381-ede7a5edcce8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train period: 202001 → 202301\n",
            "Test  period: 202302 → 202303\n",
            "\n",
            "GBM Model Evaluation:\n",
            "R-squared: 0.8978\n",
            "Mean Squared Error: 0.1920\n",
            "Root Mean Squared Error: 0.4382\n",
            "\n",
            "Feature Importances:\n",
            "NVDI_squared             0.8286\n",
            "HGHT                     0.1595\n",
            "mean_NDVI_transformed    0.0119\n",
            "if_summer                0.0000\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_train = gbm_model.predict(X_train_scaled_gbm)\n",
        "print(\"Train R²:\", r2_score(y_train_gbm, y_pred_train))\n",
        "print(\"Test  R²:\", r2_score(y_test_gbm, y_pred_gbm))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUIbTssbvb4K",
        "outputId": "7915a714-0d9e-4bc0-e447-dac05862a342"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train R²: 0.905815196547818\n",
            "Test  R²: 0.8977745578446117\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Interpretation"
      ],
      "metadata": {
        "id": "HZ7Mo9Vbuxja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Interpretation: How NDVI Explains the Difference Between ERA5 Temperatures and Weather Station Data**\n",
        "\n",
        "### **1. NDVI captures land surface characteristics that ERA5 cannot fully resolve**\n",
        "\n",
        "ERA5 temperature is produced at coarse spatial resolution, while weather stations measure near-surface conditions directly affected by vegetation, soil moisture, shading, and surface roughness. NDVI represents vegetation density and greenness, which strongly influence localized heat exchange. Therefore, NDVI provides fine-scale information that helps explain the discrepancy between ERA5 and station observations.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Both linear and GBM models show that NDVI has a *nonlinear* effect**\n",
        "\n",
        "In the linear model, **NDVI² is strongly significant**, while the linear NDVI term is negative. This pattern indicates an **inverted-U relationship**:\n",
        "\n",
        "* At low NDVI, more vegetation increases the ERA5–station temperature difference.\n",
        "* Beyond a threshold, higher NDVI reduces the temperature difference.\n",
        "\n",
        "This implies that ERA5 systematically misrepresents temperature in areas with intermediate vegetation but aligns better in densely vegetated regions.\n",
        "\n",
        "The GBM results reinforce this: **NDVI² contributes ~83% of the model’s predictive power**, dominating all other features. This confirms that the true relationship between vegetation and temperature bias is highly nonlinear.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. NDVI explains microclimate-driven station–ERA5 discrepancies**\n",
        "\n",
        "Vegetation modifies near-surface temperatures through evapotranspiration, shading, and changes in albedo. Stations located in vegetated environments experience localized cooling or warming that ERA5’s coarse grid cannot capture. As vegetation density increases, ERA5 errors change direction and magnitude, producing the observed curvature.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Model performance confirms NDVI’s explanatory value**\n",
        "\n",
        "The time-based GBM achieves **R² ≈ 0.90**, showing NDVI (especially NDVI²) robustly explains a large share of ERA5–station temperature differences, even on future data.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "\n",
        "NDVI, particularly its nonlinear form, is a **key predictor** of temperature discrepancies between ERA5 and ground observations. It captures critical local vegetation effects missing from reanalysis data, enabling substantially more accurate bias correction.\n"
      ],
      "metadata": {
        "id": "UyBE5yhTuy_p"
      }
    }
  ]
}